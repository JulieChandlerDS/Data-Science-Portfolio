{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulieChandlerDS/Data-Science-Portfolio/blob/main/Text_Generation_(RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVJKaxoLgFpU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgur46qigNbs"
      },
      "outputs": [],
      "source": [
        "#Space Rats of the CCC by Harry Harrison\n",
        "#path_to_file = tf.keras.utils.get_file('ccc.txt', 'https://raw.githubusercontent.com/JulieChandlerDS/spaceratsccc/main/ccc.txt')\n",
        "#once and future king\n",
        "#path_to_file = tf.keras.utils.get_file('theonceandfutureking.txt', 'https://raw.githubusercontent.com/JulieChandler-DS/onceandfutureking/main/onceandfutureking.txt')\n",
        "#All your Language are Destroyed by us\n",
        "#path_to_file = tf.keras.utils.get_file('RNN Fanfic Trainer.txt', 'https://raw.githubusercontent.com/JulieChandlerDS/ayladbu/main/RNN%20fanfic%20trainer.txt')\n",
        "path_to_file = tf.keras.utils.get_file('Clownpost.txt', 'https://raw.githubusercontent.com/JulieChandlerDS/ayladbu/main/Clownpost.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V2XwlrfgmxW"
      },
      "outputs": [],
      "source": [
        "#Read and decode txt for file compat\n",
        "#encoding utf-8 seems to work for all txt files\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of   text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPbV81q_gqkQ"
      },
      "outputs": [],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m48f_Oftgsaf"
      },
      "outputs": [],
      "source": [
        "#View unique characters\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG6r53gigujP"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVBZVr-lgxEh"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaVmxCfegzLT"
      },
      "outputs": [],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz_eO5Byg08D"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meuG8WZ_g2--"
      },
      "outputs": [],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQibQZFOg4lR"
      },
      "outputs": [],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB1T0e9gg6dp"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE_sDb6sg8K9"
      },
      "outputs": [],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG6v6xzgg-8j"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If7aXXq_hAqA"
      },
      "outputs": [],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBfij1x2hIrD"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-J2-_iChDGr"
      },
      "outputs": [],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsE61jKPhJp9"
      },
      "outputs": [],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqo8NGc4hNDs"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtTNkblkhOyX"
      },
      "outputs": [],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPsdThDIhQeY"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4XqMKbghSAp"
      },
      "outputs": [],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWgwEXB2hT7R"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 17926\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNOwfeZehWaX"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_chLBXnha9i"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlRJfX9shdBO"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmFAHIsPhez5"
      },
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FInU2X8Ohkww"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg_drzMxhmwI"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXHEOvuOhoaU"
      },
      "outputs": [],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vur0FGKYhp1_"
      },
      "outputs": [],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGt9xi07hsDM"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzxOcmyshwrI"
      },
      "outputs": [],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc_atkfrhzAu"
      },
      "outputs": [],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSKoidBUh0uh"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCP4KsmDh3eC"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPgvSEznh5et"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlki7zJQh7Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0f5b62-f57a-46db-e647-058773e03ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "4/4 [==============================] - 24s 5s/step - loss: 4.2395\n",
            "Epoch 2/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 4.9218\n",
            "Epoch 3/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 3.9170\n",
            "Epoch 4/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 3.7160\n",
            "Epoch 5/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 3.5454\n",
            "Epoch 6/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 3.3099\n",
            "Epoch 7/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 3.1415\n",
            "Epoch 8/120\n",
            "4/4 [==============================] - 22s 6s/step - loss: 3.1070\n",
            "Epoch 9/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 3.0294\n",
            "Epoch 10/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.9736\n",
            "Epoch 11/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.9119\n",
            "Epoch 12/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.8376\n",
            "Epoch 13/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.7746\n",
            "Epoch 14/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.7022\n",
            "Epoch 15/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.6380\n",
            "Epoch 16/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.5957\n",
            "Epoch 17/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.5519\n",
            "Epoch 18/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.5149\n",
            "Epoch 19/120\n",
            "4/4 [==============================] - 22s 6s/step - loss: 2.4892\n",
            "Epoch 20/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.4656\n",
            "Epoch 21/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.4355\n",
            "Epoch 22/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.4189\n",
            "Epoch 23/120\n",
            "4/4 [==============================] - 22s 6s/step - loss: 2.4036\n",
            "Epoch 24/120\n",
            "4/4 [==============================] - 22s 6s/step - loss: 2.3864\n",
            "Epoch 25/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.3695\n",
            "Epoch 26/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.3563\n",
            "Epoch 27/120\n",
            "4/4 [==============================] - 22s 6s/step - loss: 2.3424\n",
            "Epoch 28/120\n",
            "4/4 [==============================] - 23s 6s/step - loss: 2.3264\n",
            "Epoch 29/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.3207\n",
            "Epoch 30/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.2999\n",
            "Epoch 31/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2902\n",
            "Epoch 32/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2771\n",
            "Epoch 33/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2615\n",
            "Epoch 34/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2526\n",
            "Epoch 35/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2400\n",
            "Epoch 36/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2182\n",
            "Epoch 37/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.2156\n",
            "Epoch 38/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1987\n",
            "Epoch 39/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1847\n",
            "Epoch 40/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1713\n",
            "Epoch 41/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1609\n",
            "Epoch 42/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1437\n",
            "Epoch 43/120\n",
            "4/4 [==============================] - 22s 5s/step - loss: 2.1339\n",
            "Epoch 44/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1164\n",
            "Epoch 45/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.1049\n",
            "Epoch 46/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.0875\n",
            "Epoch 47/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 2.0740\n",
            "Epoch 48/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 2.0594\n",
            "Epoch 49/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.0450\n",
            "Epoch 50/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.0254\n",
            "Epoch 51/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 2.0158\n",
            "Epoch 52/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.9994\n",
            "Epoch 53/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.9867\n",
            "Epoch 54/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.9679\n",
            "Epoch 55/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 1.9526\n",
            "Epoch 56/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.9340\n",
            "Epoch 57/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.9156\n",
            "Epoch 58/120\n",
            "4/4 [==============================] - 23s 6s/step - loss: 1.9026\n",
            "Epoch 59/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.8856\n",
            "Epoch 60/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 1.8649\n",
            "Epoch 61/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.8490\n",
            "Epoch 62/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.8304\n",
            "Epoch 63/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.8070\n",
            "Epoch 64/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.7941\n",
            "Epoch 65/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.7711\n",
            "Epoch 66/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.7508\n",
            "Epoch 67/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.7317\n",
            "Epoch 68/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.7050\n",
            "Epoch 69/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 1.6798\n",
            "Epoch 70/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.6604\n",
            "Epoch 71/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 1.6339\n",
            "Epoch 72/120\n",
            "4/4 [==============================] - 20s 5s/step - loss: 1.6127\n",
            "Epoch 73/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.5812\n",
            "Epoch 74/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.5564\n",
            "Epoch 75/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.5217\n",
            "Epoch 76/120\n",
            "4/4 [==============================] - 21s 5s/step - loss: 1.4992\n",
            "Epoch 77/120\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RjokfQth9OC"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMncmMicrkoe"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FweDPo1xuy2w"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\"U should be my clown gf'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(5000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text Generation (RNN).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZbpsDasnWF+K1bzjOQz+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}